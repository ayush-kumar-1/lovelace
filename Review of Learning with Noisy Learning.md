# Learning with Noisy Labels 
Deep neural networks (DNN) have a tendency to memorize and overfit. Arpit. et. al showed that DNNs will first memorize general patterns, and only later overfit on noisy labels. [@arpitCloserLookMemorization2017].  Real world data is prone is noisy labels and requires approaches leverage large-scale noisy datasets. Waseem et. al showed that cyberbullying annotations can  suffer from biased label annotations [@waseemAreYouRacist2016]. Learning with noisy labels (LNL) is a recent topic in deep learning research that attempts to limit the effect of noisy labels. Most papers divide these into two approaches, sample selection and loss function regularization. Song et. al provides a more robust survey on the topic categorizing a good portion of state-of-the-art (SOTA) approaches into five categories: robust architecture, sample selection, robust loss function, loss adjustment, and robust regularization [@songLearningNoisyLabels2022]. This document details a diverse selection of approaches in the context of automatic cyberbullying detection. The problem of text classification diverges from most of existing literature which focuses on label noise in image classification datasets, most commonly CIFAR-10, CIFAR-100, MNIST, and Clothing1M. 

The first portion of this document will lay the preliminaries for LNL problem. Then we will sample some of the approaches available to address the issue. Afte